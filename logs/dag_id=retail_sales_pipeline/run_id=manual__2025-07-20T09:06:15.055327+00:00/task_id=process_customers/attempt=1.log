[2025-07-20T09:06:17.212+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: retail_sales_pipeline.process_customers manual__2025-07-20T09:06:15.055327+00:00 [queued]>
[2025-07-20T09:06:17.238+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: retail_sales_pipeline.process_customers manual__2025-07-20T09:06:15.055327+00:00 [queued]>
[2025-07-20T09:06:17.238+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2025-07-20T09:06:17.238+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 2
[2025-07-20T09:06:17.238+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2025-07-20T09:06:17.354+0000] {taskinstance.py:1304} INFO - Executing <Task(BashOperator): process_customers> on 2025-07-20 09:06:15.055327+00:00
[2025-07-20T09:06:17.398+0000] {standard_task_runner.py:55} INFO - Started process 267 to run task
[2025-07-20T09:06:17.404+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'retail_sales_pipeline', 'process_customers', 'manual__2025-07-20T09:06:15.055327+00:00', '--job-id', '3', '--raw', '--subdir', 'DAGS_FOLDER/retail_sales_pipeline.py', '--cfg-path', '/tmp/tmpp40aitie']
[2025-07-20T09:06:17.409+0000] {standard_task_runner.py:83} INFO - Job 3: Subtask process_customers
[2025-07-20T09:06:18.007+0000] {task_command.py:389} INFO - Running <TaskInstance: retail_sales_pipeline.process_customers manual__2025-07-20T09:06:15.055327+00:00 [running]> on host fbabcc26ebe4
[2025-07-20T09:06:18.335+0000] {taskinstance.py:1513} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=retail_sales_pipeline
AIRFLOW_CTX_TASK_ID=process_customers
AIRFLOW_CTX_EXECUTION_DATE=2025-07-20T09:06:15.055327+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-07-20T09:06:15.055327+00:00
[2025-07-20T09:06:18.337+0000] {subprocess.py:63} INFO - Tmp dir root location: 
 /tmp
[2025-07-20T09:06:18.338+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', '\n        docker exec spark-master /opt/bitnami/spark/bin/spark-submit             /opt/***/scripts/ingestion/process_customers.py\n        ']
[2025-07-20T09:06:18.350+0000] {subprocess.py:86} INFO - Output:
[2025-07-20T09:06:27.235+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SparkContext: Running Spark version 3.5.5
[2025-07-20T09:06:27.246+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SparkContext: OS info Linux, 6.10.14-linuxkit, amd64
[2025-07-20T09:06:27.246+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SparkContext: Java version 17.0.14
[2025-07-20T09:06:27.367+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO ResourceUtils: ==============================================================
[2025-07-20T09:06:27.370+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-07-20T09:06:27.372+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO ResourceUtils: ==============================================================
[2025-07-20T09:06:27.375+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SparkContext: Submitted application: ProcessCustomers
[2025-07-20T09:06:27.441+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-07-20T09:06:27.451+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO ResourceProfile: Limiting resource is cpu
[2025-07-20T09:06:27.453+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-07-20T09:06:27.574+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SecurityManager: Changing view acls to: spark
[2025-07-20T09:06:27.576+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SecurityManager: Changing modify acls to: spark
[2025-07-20T09:06:27.578+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SecurityManager: Changing view acls groups to:
[2025-07-20T09:06:27.578+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SecurityManager: Changing modify acls groups to:
[2025-07-20T09:06:27.579+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2025-07-20T09:06:27.749+0000] {subprocess.py:93} INFO - 25/07/20 09:06:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-20T09:06:28.506+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO Utils: Successfully started service 'sparkDriver' on port 37363.
[2025-07-20T09:06:28.569+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO SparkEnv: Registering MapOutputTracker
[2025-07-20T09:06:28.658+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO SparkEnv: Registering BlockManagerMaster
[2025-07-20T09:06:28.694+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-07-20T09:06:28.695+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-07-20T09:06:28.707+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-07-20T09:06:28.751+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8940c0b0-93ef-4ffe-8a0a-6300ab3a673f
[2025-07-20T09:06:28.777+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-07-20T09:06:28.796+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-07-20T09:06:28.996+0000] {subprocess.py:93} INFO - 25/07/20 09:06:28 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-07-20T09:06:29.086+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-07-20T09:06:29.275+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO Executor: Starting executor ID driver on host 2c1c54a40160
[2025-07-20T09:06:29.277+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO Executor: OS info Linux, 6.10.14-linuxkit, amd64
[2025-07-20T09:06:29.278+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO Executor: Java version 17.0.14
[2025-07-20T09:06:29.285+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-07-20T09:06:29.287+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@652fb983 for default.
[2025-07-20T09:06:29.459+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35621.
[2025-07-20T09:06:29.459+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO NettyBlockTransferService: Server created on 2c1c54a40160:35621
[2025-07-20T09:06:29.466+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-07-20T09:06:29.477+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2c1c54a40160, 35621, None)
[2025-07-20T09:06:29.485+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO BlockManagerMasterEndpoint: Registering block manager 2c1c54a40160:35621 with 434.4 MiB RAM, BlockManagerId(driver, 2c1c54a40160, 35621, None)
[2025-07-20T09:06:29.487+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2c1c54a40160, 35621, None)
[2025-07-20T09:06:29.489+0000] {subprocess.py:93} INFO - 25/07/20 09:06:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2c1c54a40160, 35621, None)
[2025-07-20T09:06:30.827+0000] {subprocess.py:93} INFO - 25/07/20 09:06:30 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-07-20T09:06:30.844+0000] {subprocess.py:93} INFO - 25/07/20 09:06:30 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2025-07-20T09:06:32.539+0000] {subprocess.py:93} INFO - 25/07/20 09:06:32 INFO InMemoryFileIndex: It took 108 ms to list leaf files for 1 paths.
[2025-07-20T09:06:32.711+0000] {subprocess.py:93} INFO - 25/07/20 09:06:32 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 1 paths.
[2025-07-20T09:06:37.474+0000] {subprocess.py:93} INFO - 25/07/20 09:06:37 INFO FileSourceStrategy: Pushed Filters:
[2025-07-20T09:06:37.481+0000] {subprocess.py:93} INFO - 25/07/20 09:06:37 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2025-07-20T09:06:38.616+0000] {subprocess.py:93} INFO - 25/07/20 09:06:38 INFO CodeGenerator: Code generated in 469.800334 ms
[2025-07-20T09:06:38.989+0000] {subprocess.py:93} INFO - 25/07/20 09:06:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.6 KiB, free 434.2 MiB)
[2025-07-20T09:06:39.266+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 434.2 MiB)
[2025-07-20T09:06:39.279+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2c1c54a40160:35621 (size: 34.3 KiB, free: 434.4 MiB)
[2025-07-20T09:06:39.296+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO SparkContext: Created broadcast 0 from csv at <unknown>:0
[2025-07-20T09:06:39.340+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-20T09:06:39.607+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO SparkContext: Starting job: csv at <unknown>:0
[2025-07-20T09:06:39.641+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO DAGScheduler: Got job 0 (csv at <unknown>:0) with 1 output partitions
[2025-07-20T09:06:39.643+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO DAGScheduler: Final stage: ResultStage 0 (csv at <unknown>:0)
[2025-07-20T09:06:39.644+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO DAGScheduler: Parents of final stage: List()
[2025-07-20T09:06:39.645+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO DAGScheduler: Missing parents: List()
[2025-07-20T09:06:39.654+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0), which has no missing parents
[2025-07-20T09:06:39.824+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)
[2025-07-20T09:06:39.834+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)
[2025-07-20T09:06:39.836+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2c1c54a40160:35621 (size: 6.4 KiB, free: 434.4 MiB)
[2025-07-20T09:06:39.844+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-07-20T09:06:39.891+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-20T09:06:39.893+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-07-20T09:06:39.975+0000] {subprocess.py:93} INFO - 25/07/20 09:06:39 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2c1c54a40160, executor driver, partition 0, PROCESS_LOCAL, 9610 bytes)
[2025-07-20T09:06:40.014+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-07-20T09:06:40.367+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO CodeGenerator: Code generated in 51.872042 ms
[2025-07-20T09:06:40.385+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO FileScanRDD: Reading File path: file:///opt/***/data/customers/customers.csv, range: 0-3955, partition values: [empty row]
[2025-07-20T09:06:40.462+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO CodeGenerator: Code generated in 48.201125 ms
[2025-07-20T09:06:40.679+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1707 bytes result sent to driver
[2025-07-20T09:06:40.728+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 775 ms on 2c1c54a40160 (executor driver) (1/1)
[2025-07-20T09:06:40.735+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-07-20T09:06:40.745+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO DAGScheduler: ResultStage 0 (csv at <unknown>:0) finished in 1.068 s
[2025-07-20T09:06:40.760+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-20T09:06:40.762+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-07-20T09:06:40.770+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO DAGScheduler: Job 0 finished: csv at <unknown>:0, took 1.160124 s
[2025-07-20T09:06:40.831+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO CodeGenerator: Code generated in 29.530625 ms
[2025-07-20T09:06:40.961+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO FileSourceStrategy: Pushed Filters:
[2025-07-20T09:06:40.963+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-20T09:06:40.982+0000] {subprocess.py:93} INFO - 25/07/20 09:06:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.6 KiB, free 434.0 MiB)
[2025-07-20T09:06:41.019+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)
[2025-07-20T09:06:41.023+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2c1c54a40160:35621 (size: 34.3 KiB, free: 434.3 MiB)
[2025-07-20T09:06:41.027+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO SparkContext: Created broadcast 2 from csv at <unknown>:0
[2025-07-20T09:06:41.032+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-20T09:06:41.235+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO SparkContext: Starting job: csv at <unknown>:0
[2025-07-20T09:06:41.237+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Got job 1 (csv at <unknown>:0) with 1 output partitions
[2025-07-20T09:06:41.238+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Final stage: ResultStage 1 (csv at <unknown>:0)
[2025-07-20T09:06:41.238+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Parents of final stage: List()
[2025-07-20T09:06:41.239+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Missing parents: List()
[2025-07-20T09:06:41.241+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at <unknown>:0), which has no missing parents
[2025-07-20T09:06:41.311+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.7 KiB, free 433.9 MiB)
[2025-07-20T09:06:41.352+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.7 KiB, free 433.9 MiB)
[2025-07-20T09:06:41.354+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2c1c54a40160:35621 (size: 12.7 KiB, free: 434.3 MiB)
[2025-07-20T09:06:41.358+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-07-20T09:06:41.362+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-20T09:06:41.363+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-07-20T09:06:41.367+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (2c1c54a40160, executor driver, partition 0, PROCESS_LOCAL, 9610 bytes)
[2025-07-20T09:06:41.371+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2025-07-20T09:06:41.391+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2c1c54a40160:35621 in memory (size: 34.3 KiB, free: 434.3 MiB)
[2025-07-20T09:06:41.412+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2c1c54a40160:35621 in memory (size: 6.4 KiB, free: 434.4 MiB)
[2025-07-20T09:06:41.507+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO CodeGenerator: Code generated in 30.35675 ms
[2025-07-20T09:06:41.513+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO FileScanRDD: Reading File path: file:///opt/***/data/customers/customers.csv, range: 0-3955, partition values: [empty row]
[2025-07-20T09:06:41.655+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1717 bytes result sent to driver
[2025-07-20T09:06:41.698+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 328 ms on 2c1c54a40160 (executor driver) (1/1)
[2025-07-20T09:06:41.703+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-07-20T09:06:41.718+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: ResultStage 1 (csv at <unknown>:0) finished in 0.464 s
[2025-07-20T09:06:41.737+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-20T09:06:41.744+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-07-20T09:06:41.756+0000] {subprocess.py:93} INFO - 25/07/20 09:06:41 INFO DAGScheduler: Job 1 finished: csv at <unknown>:0, took 0.519363 s
[2025-07-20T09:06:42.070+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO FileSourceStrategy: Pushed Filters:
[2025-07-20T09:06:42.071+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO FileSourceStrategy: Post-Scan Filters:
[2025-07-20T09:06:42.162+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-20T09:06:42.261+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-20T09:06:42.262+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-20T09:06:42.264+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-20T09:06:42.265+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-20T09:06:42.266+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-20T09:06:42.267+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-20T09:06:42.528+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 199.5 KiB, free 433.9 MiB)
[2025-07-20T09:06:42.570+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 2c1c54a40160:35621 in memory (size: 12.7 KiB, free: 434.4 MiB)
[2025-07-20T09:06:42.578+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.3 KiB, free 433.9 MiB)
[2025-07-20T09:06:42.581+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2c1c54a40160:35621 (size: 34.3 KiB, free: 434.3 MiB)
[2025-07-20T09:06:42.583+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO SparkContext: Created broadcast 4 from save at <unknown>:0
[2025-07-20T09:06:42.606+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
[2025-07-20T09:06:42.651+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO SparkContext: Starting job: save at <unknown>:0
[2025-07-20T09:06:42.654+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO DAGScheduler: Got job 2 (save at <unknown>:0) with 1 output partitions
[2025-07-20T09:06:42.655+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO DAGScheduler: Final stage: ResultStage 2 (save at <unknown>:0)
[2025-07-20T09:06:42.656+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO DAGScheduler: Parents of final stage: List()
[2025-07-20T09:06:42.659+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO DAGScheduler: Missing parents: List()
[2025-07-20T09:06:42.660+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[12] at save at <unknown>:0), which has no missing parents
[2025-07-20T09:06:42.824+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 213.2 KiB, free 433.7 MiB)
[2025-07-20T09:06:42.835+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 77.0 KiB, free 433.7 MiB)
[2025-07-20T09:06:42.838+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2c1c54a40160:35621 (size: 77.0 KiB, free: 434.3 MiB)
[2025-07-20T09:06:42.842+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-07-20T09:06:42.843+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[12] at save at <unknown>:0) (first 15 tasks are for partitions Vector(0))
[2025-07-20T09:06:42.844+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-07-20T09:06:42.850+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (2c1c54a40160, executor driver, partition 0, PROCESS_LOCAL, 9610 bytes)
[2025-07-20T09:06:42.853+0000] {subprocess.py:93} INFO - 25/07/20 09:06:42 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
[2025-07-20T09:06:43.053+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-20T09:06:43.054+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-20T09:06:43.056+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-20T09:06:43.057+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO FileOutputCommitter: File Output Committer Algorithm version is 1
[2025-07-20T09:06:43.058+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false
[2025-07-20T09:06:43.059+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
[2025-07-20T09:06:43.072+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO CodecConfig: Compression: SNAPPY
[2025-07-20T09:06:43.083+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO CodecConfig: Compression: SNAPPY
[2025-07-20T09:06:43.181+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]
[2025-07-20T09:06:43.261+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:
[2025-07-20T09:06:43.262+0000] {subprocess.py:93} INFO - {
[2025-07-20T09:06:43.262+0000] {subprocess.py:93} INFO -   "type" : "struct",
[2025-07-20T09:06:43.262+0000] {subprocess.py:93} INFO -   "fields" : [ {
[2025-07-20T09:06:43.262+0000] {subprocess.py:93} INFO -     "name" : "customer_id",
[2025-07-20T09:06:43.262+0000] {subprocess.py:93} INFO -     "type" : "integer",
[2025-07-20T09:06:43.263+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-20T09:06:43.263+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-20T09:06:43.263+0000] {subprocess.py:93} INFO -   }, {
[2025-07-20T09:06:43.263+0000] {subprocess.py:93} INFO -     "name" : "first_name",
[2025-07-20T09:06:43.263+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-07-20T09:06:43.263+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -   }, {
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -     "name" : "last_name",
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -   }, {
[2025-07-20T09:06:43.264+0000] {subprocess.py:93} INFO -     "name" : "email",
[2025-07-20T09:06:43.265+0000] {subprocess.py:93} INFO -     "type" : "string",
[2025-07-20T09:06:43.265+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-20T09:06:43.265+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-20T09:06:43.265+0000] {subprocess.py:93} INFO -   }, {
[2025-07-20T09:06:43.265+0000] {subprocess.py:93} INFO -     "name" : "signup_date",
[2025-07-20T09:06:43.266+0000] {subprocess.py:93} INFO -     "type" : "date",
[2025-07-20T09:06:43.266+0000] {subprocess.py:93} INFO -     "nullable" : true,
[2025-07-20T09:06:43.267+0000] {subprocess.py:93} INFO -     "metadata" : { }
[2025-07-20T09:06:43.267+0000] {subprocess.py:93} INFO -   } ]
[2025-07-20T09:06:43.268+0000] {subprocess.py:93} INFO - }
[2025-07-20T09:06:43.268+0000] {subprocess.py:93} INFO - and corresponding Parquet message type:
[2025-07-20T09:06:43.268+0000] {subprocess.py:93} INFO - message spark_schema {
[2025-07-20T09:06:43.269+0000] {subprocess.py:93} INFO -   optional int32 customer_id;
[2025-07-20T09:06:43.269+0000] {subprocess.py:93} INFO -   optional binary first_name (STRING);
[2025-07-20T09:06:43.270+0000] {subprocess.py:93} INFO -   optional binary last_name (STRING);
[2025-07-20T09:06:43.270+0000] {subprocess.py:93} INFO -   optional binary email (STRING);
[2025-07-20T09:06:43.270+0000] {subprocess.py:93} INFO -   optional int32 signup_date (DATE);
[2025-07-20T09:06:43.270+0000] {subprocess.py:93} INFO - }
[2025-07-20T09:06:43.270+0000] {subprocess.py:93} INFO - 
[2025-07-20T09:06:43.275+0000] {subprocess.py:93} INFO - 
[2025-07-20T09:06:43.649+0000] {subprocess.py:93} INFO - 25/07/20 09:06:43 INFO CodecPool: Got brand-new compressor [.snappy]
[2025-07-20T09:06:44.017+0000] {subprocess.py:93} INFO - 25/07/20 09:06:44 INFO FileScanRDD: Reading File path: file:///opt/***/data/customers/customers.csv, range: 0-3955, partition values: [empty row]
[2025-07-20T09:06:44.286+0000] {subprocess.py:93} INFO - 25/07/20 09:06:44 INFO CodeGenerator: Code generated in 237.277292 ms
[2025-07-20T09:06:45.228+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO FileOutputCommitter: Saved output of task 'attempt_202507200906428715164007850998874_0002_m_000000_2' to file:/opt/***/data/delta/dim_customer/_temporary/0/task_202507200906428715164007850998874_0002_m_000000
[2025-07-20T09:06:45.230+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO SparkHadoopMapRedUtil: attempt_202507200906428715164007850998874_0002_m_000000_2: Committed. Elapsed time: 4 ms.
[2025-07-20T09:06:45.281+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 2502 bytes result sent to driver
[2025-07-20T09:06:45.301+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2452 ms on 2c1c54a40160 (executor driver) (1/1)
[2025-07-20T09:06:45.303+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-07-20T09:06:45.320+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO DAGScheduler: ResultStage 2 (save at <unknown>:0) finished in 2.633 s
[2025-07-20T09:06:45.321+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-07-20T09:06:45.322+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-07-20T09:06:45.350+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO DAGScheduler: Job 2 finished: save at <unknown>:0, took 2.670119 s
[2025-07-20T09:06:45.365+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO FileFormatWriter: Start to commit write Job 54a3b861-3aec-445a-8dce-191206d393e8.
[2025-07-20T09:06:45.589+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO FileFormatWriter: Write Job 54a3b861-3aec-445a-8dce-191206d393e8 committed. Elapsed time: 219 ms.
[2025-07-20T09:06:45.596+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO FileFormatWriter: Finished processing stats for write job 54a3b861-3aec-445a-8dce-191206d393e8.
[2025-07-20T09:06:45.737+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO SparkContext: Invoking stop() from shutdown hook
[2025-07-20T09:06:45.738+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-07-20T09:06:45.772+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO SparkUI: Stopped Spark web UI at http://2c1c54a40160:4040
[2025-07-20T09:06:45.790+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-07-20T09:06:45.875+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO MemoryStore: MemoryStore cleared
[2025-07-20T09:06:45.877+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO BlockManager: BlockManager stopped
[2025-07-20T09:06:45.881+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-07-20T09:06:45.891+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-07-20T09:06:45.977+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO SparkContext: Successfully stopped SparkContext
[2025-07-20T09:06:45.978+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO ShutdownHookManager: Shutdown hook called
[2025-07-20T09:06:45.978+0000] {subprocess.py:93} INFO - 25/07/20 09:06:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-1840a5d7-1348-4a64-8597-16e2bfc03dfd
[2025-07-20T09:06:46.081+0000] {subprocess.py:93} INFO - 25/07/20 09:06:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-bf4b127f-b86f-43e0-9b40-2f9424b36e38
[2025-07-20T09:06:46.187+0000] {subprocess.py:93} INFO - 25/07/20 09:06:46 INFO ShutdownHookManager: Deleting directory /tmp/spark-1840a5d7-1348-4a64-8597-16e2bfc03dfd/pyspark-27820a24-0515-4ec5-95a4-80c17c0aeb51
[2025-07-20T09:06:46.404+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-07-20T09:06:46.714+0000] {taskinstance.py:1327} INFO - Marking task as SUCCESS. dag_id=retail_sales_pipeline, task_id=process_customers, execution_date=20250720T090615, start_date=20250720T090617, end_date=20250720T090646
[2025-07-20T09:06:46.821+0000] {local_task_job.py:159} INFO - Task exited with return code 0
[2025-07-20T09:06:46.968+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
