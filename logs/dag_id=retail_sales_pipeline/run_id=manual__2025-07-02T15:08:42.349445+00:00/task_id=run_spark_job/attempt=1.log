[2025-07-02T15:09:43.751+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: retail_sales_pipeline.run_spark_job manual__2025-07-02T15:08:42.349445+00:00 [queued]>
[2025-07-02T15:09:43.790+0000] {taskinstance.py:1087} INFO - Dependencies all met for <TaskInstance: retail_sales_pipeline.run_spark_job manual__2025-07-02T15:08:42.349445+00:00 [queued]>
[2025-07-02T15:09:43.791+0000] {taskinstance.py:1283} INFO - 
--------------------------------------------------------------------------------
[2025-07-02T15:09:43.792+0000] {taskinstance.py:1284} INFO - Starting attempt 1 of 2
[2025-07-02T15:09:43.793+0000] {taskinstance.py:1285} INFO - 
--------------------------------------------------------------------------------
[2025-07-02T15:09:43.918+0000] {taskinstance.py:1304} INFO - Executing <Task(DockerOperator): run_spark_job> on 2025-07-02 15:08:42.349445+00:00
[2025-07-02T15:09:43.988+0000] {standard_task_runner.py:55} INFO - Started process 266 to run task
[2025-07-02T15:09:44.017+0000] {standard_task_runner.py:82} INFO - Running: ['***', 'tasks', 'run', 'retail_sales_pipeline', 'run_spark_job', 'manual__2025-07-02T15:08:42.349445+00:00', '--job-id', '5', '--raw', '--subdir', 'DAGS_FOLDER/retail_sales_pipeline.py', '--cfg-path', '/tmp/tmpxs2rp1ml']
[2025-07-02T15:09:44.026+0000] {standard_task_runner.py:83} INFO - Job 5: Subtask run_spark_job
[2025-07-02T15:09:44.537+0000] {task_command.py:389} INFO - Running <TaskInstance: retail_sales_pipeline.run_spark_job manual__2025-07-02T15:08:42.349445+00:00 [running]> on host febdae18cce2
[2025-07-02T15:09:45.357+0000] {taskinstance.py:1513} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=retail_sales_pipeline
AIRFLOW_CTX_TASK_ID=run_spark_job
AIRFLOW_CTX_EXECUTION_DATE=2025-07-02T15:08:42.349445+00:00
AIRFLOW_CTX_TRY_NUMBER=1
AIRFLOW_CTX_DAG_RUN_ID=manual__2025-07-02T15:08:42.349445+00:00
[2025-07-02T15:09:45.495+0000] {docker.py:284} INFO - Starting docker container from image custom-spark:latest
[2025-07-02T15:09:46.929+0000] {docker.py:354} INFO - [38;5;6mspark [38;5;5m15:09:46.83 [0m[38;5;2mINFO [0m ==>
[2025-07-02T15:09:46.992+0000] {docker.py:354} INFO - [38;5;6mspark [38;5;5m15:09:46.97 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2025-07-02T15:09:47.048+0000] {docker.py:354} INFO - [38;5;6mspark [38;5;5m15:09:47.02 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2025-07-02T15:09:47.103+0000] {docker.py:354} INFO - [38;5;6mspark [38;5;5m15:09:47.08 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[2025-07-02T15:09:47.156+0000] {docker.py:354} INFO - [38;5;6mspark [38;5;5m15:09:47.13 [0m[38;5;2mINFO [0m ==>
[2025-07-02T15:09:47.521+0000] {docker.py:354} INFO - 
[2025-07-02T15:09:56.689+0000] {docker.py:354} INFO - 25/07/02 15:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-07-02T15:09:57.998+0000] {docker.py:354} INFO - python3: can't open file '/opt/***/scripts/transformation/process_sales_new.py': [Errno 2] No such file or directory
[2025-07-02T15:09:58.131+0000] {docker.py:354} INFO - 25/07/02 15:09:58 INFO ShutdownHookManager: Shutdown hook called
[2025-07-02T15:09:58.132+0000] {docker.py:354} INFO - 25/07/02 15:09:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-69480752-f933-465e-a46c-12429289aa15
[2025-07-02T15:09:59.222+0000] {taskinstance.py:1772} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/docker/operators/docker.py", line 430, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/docker/operators/docker.py", line 303, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.7/site-packages/airflow/providers/docker/operators/docker.py", line 359, in _run_image_with_mounts
    raise AirflowException(f"Docker container failed: {repr(result)} lines {joined_log_lines}")
airflow.exceptions.AirflowException: Docker container failed: {'StatusCode': 2} lines [38;5;6mspark [38;5;5m15:09:46.83 [0m[38;5;2mINFO [0m ==>
[38;5;6mspark [38;5;5m15:09:46.97 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[38;5;6mspark [38;5;5m15:09:47.02 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[38;5;6mspark [38;5;5m15:09:47.08 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[38;5;6mspark [38;5;5m15:09:47.13 [0m[38;5;2mINFO [0m ==>

25/07/02 15:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
python3: can't open file '/opt/***/scripts/transformation/process_sales_new.py': [Errno 2] No such file or directory
25/07/02 15:09:58 INFO ShutdownHookManager: Shutdown hook called
25/07/02 15:09:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-69480752-f933-465e-a46c-12429289aa15
[2025-07-02T15:09:59.263+0000] {taskinstance.py:1327} INFO - Marking task as UP_FOR_RETRY. dag_id=retail_sales_pipeline, task_id=run_spark_job, execution_date=20250702T150842, start_date=20250702T150943, end_date=20250702T150959
[2025-07-02T15:09:59.451+0000] {standard_task_runner.py:105} ERROR - Failed to execute job 5 for task run_spark_job (Docker container failed: {'StatusCode': 2} lines [38;5;6mspark [38;5;5m15:09:46.83 [0m[38;5;2mINFO [0m ==>
[38;5;6mspark [38;5;5m15:09:46.97 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[38;5;6mspark [38;5;5m15:09:47.02 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[38;5;6mspark [38;5;5m15:09:47.08 [0m[38;5;2mINFO [0m ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
[38;5;6mspark [38;5;5m15:09:47.13 [0m[38;5;2mINFO [0m ==>

25/07/02 15:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
python3: can't open file '/opt/***/scripts/transformation/process_sales_new.py': [Errno 2] No such file or directory
25/07/02 15:09:58 INFO ShutdownHookManager: Shutdown hook called
25/07/02 15:09:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-69480752-f933-465e-a46c-12429289aa15; 266)
[2025-07-02T15:09:59.604+0000] {local_task_job.py:159} INFO - Task exited with return code 1
[2025-07-02T15:09:59.812+0000] {taskinstance.py:2582} INFO - 0 downstream tasks scheduled from follow-on schedule check
